#
# ë§ˆí¬ë‹¤ìš´ì€ ê·¸ëƒ¥ ì—¬ê¸°ì„œ ë¶€í„° ë§ˆí¬ë‹¤ìš´ì´ê³  ì—¬ê¸°ì„œ ëë‚œë‹¤
"""
ğŸ—‚ ë°ì´í„°ì…‹: Boston Housing Dataset
ì„¤ëª…: ì§€ì—­ë³„ ë²”ì£„ìœ¨, ë°© ê°œìˆ˜, êµí†µ ì ‘ê·¼ì„± ë“±ì„ ì´ìš©í•´ ì£¼íƒ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ë¬¸ì œ ë¶„ì„ ê³¼ì •

## 1. ë°ì´í„° ì „ì²˜ë¦¬ (Data Preprocessing)
### 1.1 ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Missing Value Handling)
- Boston Housing ë°ì´í„°ì…‹ì€ ì „í†µì ìœ¼ë¡œ ê²°ì¸¡ì¹˜ê°€ ì—†ìœ¼ë‚˜, ì‹¤ì œ ìš´ì˜í™˜ê²½ì—ì„œëŠ” ëˆ„ë½ëœ ê°’ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ì´ìœ **: ê²°ì¸¡ì¹˜ëŠ” ëª¨ë¸ í•™ìŠµ ì‹œ ì •ë³´ ì†ì‹¤ì„ ì•¼ê¸°í•˜ê³  ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ë°©ë²•**:
   1. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ ê·¹íˆ ë‚®ìœ¼ë©´ í•´ë‹¹ ë ˆì½”ë“œ ì‚­ì œ
   2. ë¹„ìœ¨ì´ ë†’ê±°ë‚˜ ì¤‘ìš”í•œ ë³€ìˆ˜ì¸ ê²½ìš°, í‰ê· /ì¤‘ì•™ê°’ ëŒ€ì²´, íšŒê·€ ê¸°ë°˜ ëŒ€ì²´, KNN ëŒ€ì²´ ë“±ì„ í™œìš©
- **ë‹¤ìŒ ë‹¨ê³„**: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì§í›„ ë°ì´í„° ë¶„í¬ì™€ ëª¨ë¸ ì„±ëŠ¥ ë³€í™”ë¥¼ ì‹œê°í™”í•˜ì—¬ ì ì ˆì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.

### 1.2 ì´ìƒì¹˜ íƒì§€ ë° ì œê±° (Outlier Detection and Removal)
- íšŒê·€ ê³„ìˆ˜ì— í° ì˜í–¥ì„ ì£¼ëŠ” ì´ìƒì¹˜ëŠ” ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì €í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ì£¼ìš” ë°©ë²•**:
   1. IQR ê¸°ë°˜ Tukey ë°©ë²•: Q1 - 1.5*IQR, Q3 + 1.5*IQR ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ê°’ ì œê±°
   2. Z-score ê¸°ë°˜: ì ˆëŒ€ê°’ì´ íŠ¹ì • ì„ê³„ì¹˜(ì˜ˆ: 3)ë¥¼ ì´ˆê³¼í•˜ëŠ” ê°’ ê²€ì¶œ
   3. ì‹œê°í™”(Boxplot, Scatter)ë¡œ ì ì¬ì  ì´ìƒì¹˜ íƒìƒ‰
- **ì‹¤ìŠµ**: IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ì œê±°í•œ ë’¤, ì œê±° ì „/í›„ ëª¨ë¸ ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”.
- **ë‹¤ìŒ ë‹¨ê³„**: ì´ìƒì¹˜ ì œê±° í›„ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ì™€ ë¶„í¬ ë³€í™”ë¥¼ ê²€í† í•©ë‹ˆë‹¤.

## ë§ˆí¬ë‹¤ìš´ì€ ê·¸ëƒ¥ ì—¬ê¸°ì„œ ëë‚œë‹¤
"""
#############################
# 3. í™˜ê²½ ì„¤ì •
"""  
## 3.1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
- ë¶„ì„ì— í•„ìš”í•œ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
"""
#############################
# 3 Set-up
## 3. 1 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import cross_val_score

from sklearn import metrics
from collections import Counter
## 3.2 íšŒê·€ ì§€í‘œ í•¨ìˆ˜ ì •ì˜
def Reg_Models_Evaluation_Metrics (model,X_train,y_train,X_test,y_test,y_pred):
    cv_score = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 10)
    
    # Adjusted R-squared ê³„ì‚°
    r2 = model.score(X_test, y_test)
    # ê´€ì¸¡ì¹˜ ìˆ˜
    n = X_test.shape[0]
    # íŠ¹ì„±(ë…ë¦½ë³€ìˆ˜) ìˆ˜
    p = X_test.shape[1]
    # Adjusted R-squared ê³µì‹
    adjusted_r2 = 1-(1-r2)*(n-1)/(n-p-1)
    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))
    R2 = model.score(X_test, y_test)
    CV_R2 = cv_score.mean()

    return R2, adjusted_r2, CV_R2, RMSE
## 3.3 ë°ì´í„°ì…‹ íŠ¹ì„±

### Boston House Prices

https://www.kaggle.com/datasets/vikrishnan/boston-house-prices

ê° ë ˆì½”ë“œëŠ” ë³´ìŠ¤í„´ êµì™¸ ë˜ëŠ” ë„ì‹œë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. ë°ì´í„°ëŠ” 1970ë…„ ë³´ìŠ¤í„´ í‘œì¤€ ëŒ€ë„ì‹œ í†µê³„ ì§€ì—­(SMSA)ì—ì„œ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¹ì„±ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ (UCI ML Repository ì°¸ê³ ):

  - CRIM     ë„ì‹œë³„ 1ì¸ë‹¹ ë²”ì£„ìœ¨
  - ZN       25,000 í‰ë°©í”¼íŠ¸ ì´ìƒ ì£¼ê±°ìš© í† ì§€ ë¹„ìœ¨
  - INDUS    ë„ì‹œë³„ ë¹„ì†Œë§¤ì—… ë¹„ì¦ˆë‹ˆìŠ¤ ë©´ì  ë¹„ìœ¨
  - CHAS     ì°°ìŠ¤ê°• ë”ë¯¸ ë³€ìˆ˜(ê°•ê³¼ ì ‘í•˜ë©´ 1, ì•„ë‹ˆë©´ 0)
  - NOX      ì§ˆì†Œì‚°í™”ë¬¼ ë†ë„(1ì²œë§Œë¶„ì˜ 1)
  - RM       ì£¼íƒë‹¹ í‰ê·  ë°© ê°œìˆ˜
  - AGE      1940ë…„ ì´ì „ì— ì§€ì–´ì§„ ì†Œìœ ì£¼íƒ ë¹„ìœ¨
  - DIS      5ê°œ ë³´ìŠ¤í„´ ê³ ìš©ì„¼í„°ê¹Œì§€ì˜ ê°€ì¤‘ ê±°ë¦¬
  - RAD      ë°©ì‚¬í˜• ê³ ì†ë„ë¡œ ì ‘ê·¼ì„± ì§€ìˆ˜
  - TAX      1ë§Œ ë‹¬ëŸ¬ë‹¹ ì¬ì‚°ì„¸ìœ¨
  - PTRATIO  ë„ì‹œë³„ í•™ìƒ-êµì‚¬ ë¹„ìœ¨
  - B        1000(Bk - 0.63)^2 (Bk: í‘ì¸ ë¹„ìœ¨)
  - LSTAT    ì €ì†Œë“ì¸µ ì¸êµ¬ ë¹„ìœ¨(%)
  - MEDV     ì†Œìœ ì£¼íƒì˜ ì¤‘ì•™ê°’(ì²œ ë‹¬ëŸ¬ ë‹¨ìœ„)

ê²°ì¸¡ì¹˜: ì—†ìŒ

ì¤‘ë³µ ë°ì´í„°: ì—†ìŒ

UCI ML housing ë°ì´í„°ì…‹ ë³µì‚¬ë³¸ì…ë‹ˆë‹¤.
https://archive.ics.uci.edu/ml/machine-learning-databases/housing/
## 3.4 ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (Boston Housing)
column_names = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV']
df = pd.read_csv('housing.csv', header=None, delim_whitespace=True, names=column_names)
# 4. Some visualisations


# 4. ë°ì´í„° ì „ì²˜ë¦¬ (Data Preprocessing)
"""
## 4.1 ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Missing Value Handling)
- Boston Housing ë°ì´í„°ì…‹ì—ëŠ” ê²°ì¸¡ì¹˜ê°€ ì—†ìœ¼ë‚˜, ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” ëˆ„ë½ëœ ê°’ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **ì´ìœ **: ê²°ì¸¡ì¹˜ëŠ” í•™ìŠµ ì‹œ ì •ë³´ ì†ì‹¤ê³¼ ì˜ˆì¸¡ ì„±ëŠ¥ ì €í•˜ë¥¼ ìœ ë°œí•˜ë¯€ë¡œ ë°˜ë“œì‹œ ê²€ì¦í•©ë‹ˆë‹¤.
"""
# ê²°ì¸¡ì¹˜ í™•ì¸
print(df.isnull().sum())

"""
## 4.2 ì´ìƒì¹˜ íƒì§€ ë° ì œê±° (Outlier Detection and Removal)
- íšŒê·€ ëª¨ë¸ì˜ ì•ˆì •ì„±ì„ ìœ„í•´ IQR ê¸°ë°˜ ì´ìƒì¹˜ ì œê±°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- **ì½”ë“œ íë¦„**:
  1. ê° íŠ¹ì„±ì˜ Q1, Q3 ê³„ì‚°
  2. IQR = Q3 - Q1
  3. í•˜í•œ = Q1 - 1.5*IQR, ìƒí•œ = Q3 + 1.5*IQR
  4. í•´ë‹¹ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ìƒ˜í”Œ ì¸ë±ìŠ¤ ìˆ˜ì§‘ ë° ì œê±°
"""
def detect_outliers_iqr(data, features):
    outlier_indices = []
    for col in features:
        Q1 = np.percentile(data[col], 25)
        Q3 = np.percentile(data[col], 75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        outlier_list_col = data[(data[col] < lower) | (data[col] > upper)].index
        outlier_indices.extend(outlier_list_col)
    return list(set(outlier_indices))

numeric_features = df.columns[:-1]
outliers = detect_outliers_iqr(df, numeric_features)
print(f"ì œê±° ì „ ë°ì´í„° í¬ê¸°: {df.shape}")
df = df.drop(outliers).reset_index(drop=True)
print(f"ì œê±° í›„ ë°ì´í„° í¬ê¸°: {df.shape}")

"""
## 4.3 ë°ì´í„° ë¶„í•  (Train-Test Split)
"""
from sklearn.model_selection import train_test_split
X = df.drop('MEDV', axis=1)
y = df['MEDV']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""
## 4.4 íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (Feature Scaling)
"""
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 5. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€

# (ì´í›„ ì‹œê°í™” ë° ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ë§ ì½”ë“œëŠ” Boston Housing ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë§Œ ì§„í–‰)
# 6. Comparing different models
## 6.1 Linear Regression
from sklearn.linear_model import LinearRegression

# Creating and training model
lm = LinearRegression()
lm.fit(X_train, y_train)

# Model making a prediction on test data
y_pred = lm.predict(X_test)
### Linear Regression performance for Avocado dataset
ndf = [Reg_Models_Evaluation_Metrics(lm,X_train,y_train,X_test,y_test,y_pred)]

lm_score = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
lm_score.insert(0, 'Model', 'Linear Regression')
lm_score
plt.figure(figsize = (10,5))
sns.regplot(x=y_test,y=y_pred)
plt.title('Linear regression for Avocado dataset', fontsize = 20)
### Linear Regression performance for Boston dataset
lm.fit(X_train2, y_train2)
y_pred = lm.predict(X_test2)
ndf = [Reg_Models_Evaluation_Metrics(lm,X_train2,y_train2,X_test2,y_test2,y_pred)]

lm_score2 = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
lm_score2.insert(0, 'Model', 'Linear Regression')
lm_score2
## 6.2 Random Forest
from sklearn.ensemble import RandomForestRegressor

# Creating and training model
RandomForest_reg = RandomForestRegressor(n_estimators = 10, random_state = 0)
### Random Forest performance for Avocado dataset
RandomForest_reg.fit(X_train, y_train)
# Model making a prediction on test data
y_pred = RandomForest_reg.predict(X_test)
ndf = [Reg_Models_Evaluation_Metrics(RandomForest_reg,X_train,y_train,X_test,y_test,y_pred)]

rf_score = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
rf_score.insert(0, 'Model', 'Random Forest')
rf_score
### Random Forest performance for Boston dataset
RandomForest_reg.fit(X_train2, y_train2)
# Model making a prediction on test data
y_pred = RandomForest_reg.predict(X_test2)
ndf = [Reg_Models_Evaluation_Metrics(RandomForest_reg,X_train2,y_train2,X_test2,y_test2,y_pred)]

rf_score2 = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
rf_score2.insert(0, 'Model', 'Random Forest')
rf_score2
## 6.3 Ridge Regression
from sklearn.linear_model import Ridge

# Creating and training model
ridge_reg = Ridge(alpha=3, solver="cholesky")
### Ridge Regression performance for Avocado dataset
ridge_reg.fit(X_train, y_train)
# Model making a prediction on test data
y_pred = ridge_reg.predict(X_test)
ndf = [Reg_Models_Evaluation_Metrics(ridge_reg,X_train,y_train,X_test,y_test,y_pred)]

rr_score = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
rr_score.insert(0, 'Model', 'Ridge Regression')
rr_score
### Ridge Regression performance for Boston dataset
ridge_reg.fit(X_train2, y_train2)
# Model making a prediction on test data
y_pred = ridge_reg.predict(X_test2)
ndf = [Reg_Models_Evaluation_Metrics(ridge_reg,X_train2,y_train2,X_test2,y_test2,y_pred)]

rr_score2 = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
rr_score2.insert(0, 'Model', 'Ridge Regression')
rr_score2
## 6.4 XGBoost
from xgboost import XGBRegressor
# create an xgboost regression model
XGBR = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.8, colsample_bytree=0.8)
### XGBoost performance for Avocado dataset
XGBR.fit(X_train, y_train)
# Model making a prediction on test data
y_pred = XGBR.predict(X_test)
ndf = [Reg_Models_Evaluation_Metrics(XGBR,X_train,y_train,X_test,y_test,y_pred)]

XGBR_score = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
XGBR_score.insert(0, 'Model', 'XGBoost')
XGBR_score
### XGBoost performance for Boston dataset
XGBR.fit(X_train2, y_train2)
# Model making a prediction on test data
y_pred = XGBR.predict(X_test2)
ndf = [Reg_Models_Evaluation_Metrics(XGBR,X_train2,y_train2,X_test2,y_test2,y_pred)]

XGBR_score2 = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
XGBR_score2.insert(0, 'Model', 'XGBoost')
XGBR_score2
## 6.5 Recursive Feature Elimination (RFE)
RFE is a wrapper-type feature selection algorithm. This means that a different machine learning algorithm is given and used in the core of the method, is wrapped by RFE, and used to help select features.

Random Forest has usually good performance combining with RFE
from sklearn.feature_selection import RFE
from sklearn.pipeline import Pipeline

# create pipeline
rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=60)
model = RandomForestRegressor()
rf_pipeline = Pipeline(steps=[('s',rfe),('m',model)])
### Random Forest RFE performance for Avocado dataset
rf_pipeline.fit(X_train, y_train)
# Model making a prediction on test data
y_pred = rf_pipeline.predict(X_test)
ndf = [Reg_Models_Evaluation_Metrics(rf_pipeline,X_train,y_train,X_test,y_test,y_pred)]

rfe_score = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
rfe_score.insert(0, 'Model', 'Random Forest with RFE')
rfe_score
### Random Forest RFE performance for Boston dataset
# create pipeline
rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=8)
model = RandomForestRegressor()
rf_pipeline = Pipeline(steps=[('s',rfe),('m',model)])

rf_pipeline.fit(X_train2, y_train2)
# Model making a prediction on test data
y_pred = rf_pipeline.predict(X_test2)
ndf = [Reg_Models_Evaluation_Metrics(rf_pipeline,X_train2,y_train2,X_test2,y_test2,y_pred)]

rfe_score2 = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
rfe_score2.insert(0, 'Model', 'Random Forest with RFE')
rfe_score2
# 7. Final Model Evaluation
## 7.1 Avocado dataset
predictions = pd.concat([rfe_score, XGBR_score, rr_score, rf_score, lm_score], ignore_index=True, sort=False)
predictions
## 7.2 Boston dataset
predictions2 = pd.concat([rfe_score2, XGBR_score2, rr_score2, rf_score2, lm_score2], ignore_index=True, sort=False)
predictions2
## 7.3 Visualizing Model Performance
f, axe = plt.subplots(1,1, figsize=(18,6))

predictions.sort_values(by=['Cross Validated R2 Score'], ascending=False, inplace=True)

sns.barplot(x='Cross Validated R2 Score', y='Model', data = predictions, ax = axe)
axe.set_xlabel('Cross Validated R2 Score', size=16)
axe.set_ylabel('Model')
axe.set_xlim(0,1.0)

axe.set(title='Model Performance for Avocado dataset')

plt.show()
f, axe = plt.subplots(1,1, figsize=(18,6))

predictions2.sort_values(by=['Cross Validated R2 Score'], ascending=False, inplace=True)

sns.barplot(x='Cross Validated R2 Score', y='Model', data = predictions2, ax = axe)
axe.set_xlabel('Cross Validated R2 Score', size=16)
axe.set_ylabel('Model')
axe.set_xlim(0,1.0)

axe.set(title='Model Performance for Boston dataset')
plt.show()
# 8. Bonus: hyperparameter Tuning Using GridSearchCV
Hyperparameter tuning is the process of tuning the parameters present as the tuples while we build machine learning models. These parameters are defined by us. Machine learning algorithms never learn these parameters. These can be tuned in different step.

GridSearchCV is a technique for finding the optimal hyperparameter values from a given set of parameters in a grid. It's essentially a cross-validation technique. The model as well as the parameters must be entered. After extracting the best parameter values, predictions are made.

The â€œbestâ€ parameters that GridSearchCV identifies are technically the best that could be produced, but only by the parameters that you included in your parameter grid.
## 8.1 Tuned Ridge Regression
from sklearn.preprocessing import PolynomialFeatures

# Polynomial features are those features created by raising existing features to an exponent. 
# For example, if a dataset had one input feature X, 
# then a polynomial feature would be the addition of a new feature (column) where values were calculated by squaring the values in X, e.g. X^2.

steps = [
    ('poly', PolynomialFeatures(degree=2)),
    ('model', Ridge(alpha=3.8, fit_intercept=True))
]

ridge_pipe = Pipeline(steps)
ridge_pipe.fit(X_train, y_train)

# Model making a prediction on test data
y_pred = ridge_pipe.predict(X_test)
from sklearn.model_selection import GridSearchCV

alpha_params = [{'model__alpha': list(range(1, 15))}]

clf = GridSearchCV(ridge_pipe, alpha_params, cv = 10)
### Tuned Ridge Regression performance for Avocado dataset
# Fit and tune model
clf.fit(X_train, y_train)
# Model making a prediction on test data
y_pred = ridge_pipe.predict(X_test)
# The combination of hyperparameters along with values that give the best performance of our estimate specified
print(clf.best_params_)
ndf = [Reg_Models_Evaluation_Metrics(clf,X_train,y_train,X_test,y_test,y_pred)]

clf_score = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
clf_score.insert(0, 'Model', 'Tuned Ridge Regression')
clf_score
### Tuned Ridge Regression performance for Boston dataset
steps = [
    ('poly', PolynomialFeatures(degree=2)),
    ('model', Ridge(alpha=3.8, fit_intercept=True))
]

ridge_pipe = Pipeline(steps)
ridge_pipe.fit(X_train2, y_train2)

# Model making a prediction on test data
y_pred = ridge_pipe.predict(X_test2)

alpha_params = [{'model__alpha': list(range(1, 15))}]

clf = GridSearchCV(ridge_pipe, alpha_params, cv = 10)
# Fit and tune model
clf.fit(X_train2, y_train2)
# Model making a prediction on test data
y_pred = ridge_pipe.predict(X_test2)
# The combination of hyperparameters along with values that give the best performance of our estimate specified
print(clf.best_params_)
ndf = [Reg_Models_Evaluation_Metrics(clf,X_train2,y_train2,X_test2,y_test2,y_pred)]

clf_score2 = pd.DataFrame(data = ndf, columns=['R2 Score','Adjusted R2 Score','Cross Validated R2 Score','RMSE'])
clf_score2.insert(0, 'Model', 'Tuned Ridge Regression')
clf_score2
# 9. Final performance comparison
## 9.1 Avocado data set
result = pd.concat([clf_score, predictions], ignore_index=True, sort=False)
result
f, axe = plt.subplots(1,1, figsize=(18,6))

result.sort_values(by=['Cross Validated R2 Score'], ascending=False, inplace=True)

sns.barplot(x='Cross Validated R2 Score', y='Model', data = result, ax = axe)
#axes[0].set(xlabel='Region', ylabel='Charges')
axe.set_xlabel('Cross Validated R2 Score', size=16)
axe.set_ylabel('Model')
axe.set_xlim(0,1.0)
axe.set(title='Model Performance for Avocado dataset')

plt.show()
## 9.2 Boston data set
result = pd.concat([clf_score2, predictions2], ignore_index=True, sort=False)
result
f, axe = plt.subplots(1,1, figsize=(18,6))

result.sort_values(by=['Cross Validated R2 Score'], ascending=False, inplace=True)

sns.barplot(x='Cross Validated R2 Score', y='Model', data = result, ax = axe)
#axes[0].set(xlabel='Region', ylabel='Charges')
axe.set_xlabel('Cross Validated R2 Score', size=16)
axe.set_ylabel('Model')
axe.set_xlim(0,1.0)
axe.set(title='Model Performance for Boston dataset')

plt.show()
# 10. Other notebooks
Clustering methods - comprehensive study

https://www.kaggle.com/code/marcinrutecki/clustering-methods-comprehensive-study

Outlier detection methods!

https://www.kaggle.com/code/marcinrutecki/outlier-detection-methods

Multicollinearity - detection and remedies

https://www.kaggle.com/code/marcinrutecki/multicollinearity-detection-and-remedies
# 11. Some referrals
https://www.ncl.ac.uk/webtemplate/ask-assets/external/maths-resources/statistics/regression-and-correlation/coefficient-of-determination-r-squared.html
